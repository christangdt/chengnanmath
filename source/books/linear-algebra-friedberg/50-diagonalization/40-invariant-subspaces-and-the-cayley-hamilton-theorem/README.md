# Invariant Subspaces and the Cayley-Hamilton Theorem

### Content Notes

#### Definition (invariant subspaces)

Let $\mathsf{T}$ be a linear operator on a vector space $\mathsf{V}$. A subspace $\mathsf{W}$ of $\mathsf{V}$ is called a **$\mathsf{T}$-invariant subspace** of $\mathsf{V}$ if $\mathsf{T(W)}\subseteq\mathsf{W}$, that is, if $\mathsf{T}(v)\in\mathsf{W}$ for all $v\in\mathsf{W}$.

#### Definition ($\mathsf{T}$-cyclic subspace)

Let $\mathsf{T}$ be a linear operator on a vector space $\mathsf{V}$, and let $x$ be a nonzero vector in $\mathsf{V}$, the subspace
$$
\mathsf{W}=\text{span}(\{x,\mathsf{T}(x),\mathsf{T}^2(x),\dots\})
$$
is called the **$\mathsf{T}$-cyclic subspace** of $\mathsf{V}$ **generated by** $x$.

#### Theorem 5.21

Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $\mathsf{W}$ be a $\mathsf{T}$-invariant subspace of $\mathsf{V}$. Then the characteristic polynomial $\mathsf{T_W}$ divides the characteristic polynomial of $\mathsf{T}$​.

#### Theorem 5.22

Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $\mathsf{W}$ denote the $\mathsf{T}$-cyclic subspace of $\mathsf{V}$ generated by a nonzero vector $v\in\mathsf{V}$. Let $k=\dim(\mathsf{W})$. Then

(a) $\{v,\mathsf{T}(v),\mathsf{T}^2(v),\dots,\mathsf{T}^{k-1}(v)\}$ is a basis for $\mathsf{W}$.

(b) If $a_0v+a_1\mathsf{T}(v)+\cdots+a_{k-1}\mathsf{T}^{k-1}(v)+\mathsf{T}^k(v)=0$, then the characteristic polynomial of $\mathsf{T_W}$ is $f(t)=(-1)^k(a_0+a_1t+\cdots+a_{k-1}t^{k-1}+t^k)$​.

#### Theorem 5.23 (Cayley-Hamilton)

Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $f(t)$ be the characteristic polynomial of $\mathsf{T}$. Then $f(\mathsf{T})=\mathsf{T}_0$, the zero transformation. That is, $\mathsf{T}$ "satisfies" its characteristic equation.

##### Corollary (Cayley-Hamilton Theorem for Matrices)

Let $A$ be an $n\times n$ matrix, and let $f(t)$ be the characteristic polynomial of $A$. Then $f(A)=O$, the $n\times n$ zero matrix.

### Selected Exercises

#### 4

Write $g(t)=a_0+a_1t+\cdots+a_nt^n$, with $n$ a nonnegative integer, let $x\in\mathsf{W}$, then $\mathsf{T}^k(x)\in\mathsf{W}$ for $k=1,\dots,n$, thus
$$
g(\mathsf{T})(x)=a_0x+a_1\mathsf{T}(x)+\cdots+a_n\mathsf{T}^n(x)\in\mathsf{W}
$$

#### 5

Let $I$ be a collection of index and for each $\alpha\in I$, the set $\mathsf{W}_{\alpha}$ is a $\mathsf{T}$-invariant subspace of $\mathsf{V}$, let $\mathsf{W}=\bigcap_{\alpha\in I}\mathsf{W}_{\alpha}$, then for any $x\in\mathsf{W}$, we have $x\in\mathsf{W}_{\alpha}$ for all $\alpha\in I$, thus $\mathsf{T}(x)\in\mathsf{W}_{\alpha}$ for all $\alpha\in I$, thus $\mathsf{T}(x)\in\mathsf{W}$, and the conclusion follows.

#### 8

Since $\mathsf{T_W}(v)=\mathsf{T}(v)=\lambda v$ for $v\in\mathsf{W}\subseteq\mathsf{V}$.

#### 11

(a) For any $x\in\mathsf{W}$, we can find a nonnegative number $n$ and scalars $c_0,c_1,\dots,c_n$ such that 
$$
x=c_0v+c_1\mathsf{T}(v)+\cdots+c_n\mathsf{T}^n(v)
$$
this means
$$
\mathsf{T}(x)=c_0\mathsf{T}(v)+c_1\mathsf{T}^2(v)+\cdots+c_n\mathsf{T}^{n+1}(v)\in\mathsf{W}
$$
(b) If $\mathsf{Z}$ is a $\mathsf{T}$-invariant subspace of $\mathsf{V}$ containing $v$, then we can have $\mathsf{T}^k(v)\in\mathsf{Z}$ for all integers $k$, thus any $x\in\mathsf{W}$ is a linear combination of vectors in $\mathsf{Z}$, thus $x\in\mathsf{Z}$ and $\mathsf{W}\subseteq\mathsf{Z}$​.

#### 14

From Theorem 5.22 we know that $\{v,\mathsf{T}(v),\mathsf{T}^2(v),\dots,\mathsf{T}^{k-1}(v)\}$ is a basis for $\mathsf{W}$ if we let $k=\dim(\mathsf{W})$. If given $w\in\mathsf{W}$, there exists a polynomial $g(t)$ such that $w=g(\mathsf{T})(v)$ and $\deg(g)>k$, then each of the items $\mathsf{T}^{k+1}(v),\dots,\mathsf{T}^{\deg(g)}(v)$ can all be expressed as a linear combination of $v,\mathsf{T}(v),\mathsf{T}^2(v),\dots,\mathsf{T}^{k-1}(v)$, thus we can rewrite $g(\mathsf{T})$ to be $g'(\mathsf{T})$ with $\deg(g')\le k$.

#### 15

Let $\mathsf{F}^n$ be the $n$-tuple vector space and $\beta$ the standard ordered basis for it, consider the linear transformation $\mathsf{T}=\mathsf{L}_A$, we have $A=[\mathsf{T}]_{\beta}$ and $f(A)=[f(\mathsf{T})]_{\beta}=[\mathsf{T}_0]_{\beta}=O$.

#### 16

(a) Since the characteristic polynomial of the restriction of $\mathsf{T}$ to a $\mathsf{T}$-invariant subspace of $\mathsf{V}$ divides the original characteristic polynomial, by Theorem 5.21

(b) If $\mathsf{W}$ is a nontrivial $\mathsf{T}$-invariant subspace of $\mathsf{V}$, then the characteristic polynomial of $\mathsf{T_W}$ has degree more than 1, thus must contain a root $\lambda$, let $v$ be an eigenvector of $\mathsf{T_W}$ corresponding to $\lambda$, by Exercise 8, $v$ is also an eigenvector of $\mathsf{T}$ corresponding to $\lambda$.

#### 17

Let $f(t)$ be the characteristic polynomial of $A$, then $f(A)=O$ by Cayley-Hamiltion Theorem, since the coefficient of $t^n$ in $f(t)$ is $(-1)^n$, we see that $A^n$ is a linear combination of $I_n,A,\dots,A^{n-1}$, suppose for $k\ge n$ we already have $A^k$ being a linear combination of $I_n,A,\dots,A^{n-1}$, consider $A^{k+1}$, we can write $A^{k+1}=AA^k$, where
$$
A^k=c_0I_n+c_1A+\cdots+c_{n-1}A^{n-1},\quad c_0,c_1,\dots,c_{n-1}\in F
$$
then we have
$$
\begin{aligned}
A^{k+1}&=A(c_0I_n+c_1A+\cdots+c_{n-1}A^{n-1})=c_0A+c_1A^2+\cdots+c_{n-1}A^n
\\&=c_0A+c_1A^2+\cdots+c_{n-2}A^{n-1}+c_{n-1}(d_0I_n+d_1A+\cdots+d_{n-1}A^{n-1})
\\&=c_{n-1}d_0I_n+(c_1+c_{n-1}d_1)A+\cdots+(c_{n-2}+c_{n-1}d_{n-2})A^{n-2}+c_{n-1}d_{n-1}A^{n-1}
\end{aligned}
$$
for some scalars $d_0,\dots,d_{n-1}\in F$, thus $A^{k+1}\in\text{span}(I_n,A,\dots,A^{n-1})$. It follows that
$$
\text{span}(I_n,A,A^2,\dots)\subseteq\text{span}(I_n,A,\dots,A^{n-1})
$$
thus
$$
\dim(\text{span}(I_n,A,A^2,\dots))\le\dim(\text{span}(I_n,A,\dots,A^{n-1}))\le n
$$

#### 18

(a) $A$ is invertible if and only if $0$ is not a root of $f(t)$, if and only if $a_0\ne0$.

(b) Since $f(A)=O$ by Cayley-Hamiltion Theorem, we have
$$
f(A)=(-1)^nA^n+a_{n-1}A^{n-1}+\cdots+a_1A+a_0I_n\\\implies[(-1)^nA^{n-1}+\cdots+a_1I_n]A=-a_0I_n
$$
this means $(-1/a_0)[(-1)^nA^{n-1}+\cdots+a_1I_n]$ is a left inverse of $A$. Since $A$ is invertible, the inverse of $A$ is equal to its left and right inverse.

#### 19

Use induction on $k$. If $k=1$, then $A=(-a_0)$, and the characteristic polynomial of $A$ is $-a_0-t=(-1)^1(a_0+t^1)$, so the induction hypothesis holds. Suppose the conclusion is true for the $k\times k$ matrix. Let $A$ be the $(k+1)\times(k+1)$ matrix
$$
A:=\begin{pmatrix}
0&0&\cdots&0&-a_0
\\
1&0&\cdots&0&-a_1
\\
0&1&\cdots&0&-a_2
\\
{\vdots}&{\vdots}&&\vdots&\vdots
\\
0&0&\cdots&0&-a_{k-1}
\\
0&0&\cdots&1&-a_{k}
\end{pmatrix}
$$
then the characteristic polynomial of $A$ is
$$
f(t)=\det(A-tI_{k+1})=\det
\begin{pmatrix}
-t&0&\cdots&0&-a_0
\\
1&-t&\cdots&0&-a_1
\\
0&1&\cdots&0&-a_2
\\
{\vdots}&{\vdots}&&\vdots&\vdots
\\
0&0&\cdots&-t&-a_{k-1}
\\
0&0&\cdots&1&-a_{k}-t
\end{pmatrix}
\\=-t\det\begin{pmatrix}
-t&0&\cdots&0&-a_1
\\
1&-t&\cdots&0&-a_2
\\
{\vdots}&{\vdots}&&\vdots&\vdots
\\
0&0&\cdots&-t&-a_{k-1}
\\
0&0&\cdots&1&t-a_{k}
\end{pmatrix}+(-1)^{k+2}(-a_0)
$$
the first item, use the induction hypothesis, is equal to $-t$ times the characteristic polynomial of the matrix
$$
\begin{pmatrix}
0&0&\cdots&0&-a_1
\\
1&0&\cdots&0&-a_2
\\
{\vdots}&{\vdots}&&\vdots&\vdots
\\
0&0&\cdots&0&-a_{k-1}
\\
0&0&\cdots&1&-a_{k}
\end{pmatrix}
$$
which is equal to $(-1)^k(a_1+a_2t+\cdots+a_kt^{k-1}+t^k)$, thus we get
$$
f(t)=(-1)^{k+1}(a_1t+a_2t^2+\cdots+a_kt^k+t^{k+1})+(-1)^{k+1}a_0
$$
thus the induction hypothesis holds.

#### 20

If $\mathsf{U}=g(\mathsf{T})$ for some polynomial $g(t)$, then it is easy to see that $\mathsf{UT}=\mathsf{TU}$. Conversely, suppose $\mathsf{V}$ is generated by $v$, by Exercise 13 we can choose a polynomial $g(t)$ such that $\mathsf{U}(v)=\mathsf{g(\mathsf{T})}(v)$, since $\mathsf{V}$ is a $\mathsf{T}$-cyclic subspace of itself, for any $x\in\mathsf{V}$, we can write $x=a_0v+a_1\mathsf{T}(v)+\cdots+a_n\mathsf{T}^n(v)$ for scalars $a_0,a_1,\dots,a_n\in F$, thus
$$
\begin{aligned}
\mathsf{U}(x)&=\mathsf{U}(a_0v+a_1\mathsf{T}(v)+\cdots+a_n\mathsf{T}^n(v))
\\&=a_0\mathsf{U}(v)+a_1\mathsf{U}\mathsf{T}(v)+\cdots+a_n\mathsf{U}\mathsf{T}^n(v)
\\&=a_0\mathsf{U}(v)+a_1\mathsf{T}\mathsf{U}(v)+\cdots+a_n\mathsf{T}^n\mathsf{U}(v)\qquad(\text{since }\mathsf{UT=TU})
\\&=(a_0\mathsf{I_V}+a_1\mathsf{T}+\cdots+a_n\mathsf{T}^n)g(\mathsf{T})(v)
\\&=g(\mathsf{T})(a_0\mathsf{I_V}+a_1\mathsf{T}+\cdots+a_n\mathsf{T}^n)(v)
\\&=g(\mathsf{T})(x)
\end{aligned}
$$
since $x$ is arbitrary, we conclude $\mathsf{U}=g(\mathsf{T})$ on $\mathsf{V}$​.

#### 21

If $\mathsf{V}$ is not a $\mathsf{T}$-cyclic subspace, then for all $v\in\mathsf{V}$, we have $\mathsf{T}(v)\in\text{span}(v)$, for otherwise $\{v,\mathsf{T}(v)\}$ would be linearly independent, thus a basis for $\mathsf{V}$. To prove $\mathsf{T}=c\mathsf{I}$, choose any distinct vectors $u,v\in\mathsf{V}$ such that $\mathsf{T}(v)=cv$ and $\mathsf{T}(u)=du$, and assume $c\ne d$, since $v-u\ne0$, we have $\mathsf{T}(v-u)=e(v-u)$ for some scalar $e$, this means
$$
e(v-u)=cv-du\implies(e-c)v+(d-e)u=0
$$
since $u$ and $v$ are eigenvectors corresponding to different eigenvalues of $\mathsf{T}$, they are linearly independent, thus $e-c=d-e=0$, which means $c=d$​, a contradiction.

#### 22

Use Exercise 21, if $\mathsf{T}\ne c\mathsf{I}$, then $\mathsf{V}$ is a $\mathsf{T}$-cyclic subspace, then use Exercise 20.

#### 23

Induction on $k$, the conclusion is true for $k=1$, suppose the conclusion is true for $k$, consider $v_1,\dots,v_{k+1}$ such that $v_1+\cdots+v_{k+1}\in\mathsf{W}$, suppose $\mathsf{T}(v_i)=\lambda_i v_i$ for all $i$, then
$$
\mathsf{T}(v_1+\cdots+v_{k+1})=\sum_{i=1}^k\lambda_i v_i+\lambda_{k+1}v_{k+1}\in\mathsf{W}
$$
We further have $\lambda_{k+1}(v_1+\cdots+v_{k+1})\in\mathsf{W}$, this gives $\sum_{i=1}^k(\lambda_i-\lambda_{k+1})v_i\in\mathsf{W}$, since each $(\lambda_i-\lambda_{k+1})v_i$ is an eigenvector of $\mathsf{T}$ corresponding to $\lambda_i$, use the induction hypothesis we see that each $(\lambda_i-\lambda_{k+1})v_i$ belongs to $\mathsf{W}$, or $v_i\in\mathsf{W}$ for $i=1,\dots,k$, then we have $v_{k+1}\in\mathsf{W}$, thus the conclusion holds.

#### 24

Since $\mathsf{T}$ is diagonalizable, let $\mathsf{E}_{\lambda}$ be the eigenspace of $\mathsf{T}$ corresponding to $\lambda$, if $\mathsf{W}$ is a non-trivial $\mathsf{T}$ invariant subspace, we denote $\mathsf{W}_{\lambda}=\mathsf{E}_{\lambda}\cap\mathsf{W}$ be the eigenspace of $\mathsf{T_W}$ corresponding to $\lambda$, let $\beta_{\lambda}$ be a basis for $\mathsf{W}_{\lambda}$ and $\beta=\cup_{\lambda}\beta_{\lambda}$, by the construction of $\beta$, it is linearly independent. It suffices to show that $\beta$ spans $\mathsf{W}$. For any $v\in\mathsf{W}$, we can write $v=v_1+\cdots+v_k$, where each $v_i\in\mathsf{E}_{\lambda_i}$, use Exercise 23, each $v_i\in\mathsf{W}$, thus $v_i\in\mathsf{W}_{\lambda_i}$, so $v_i$ is a linearly combination of vectors in $\beta_{\lambda_i}$, this means $v$ is a linear combination of vectors in $\beta$.

#### 25

First for any eigenvalue $\lambda$ of $\mathsf{T}$, consider $\mathsf{E}_{\lambda}=\mathsf{N(T-\lambda I)}$, if $v\in\mathsf{E}_{\lambda}$, let $w=\mathsf{U}(v)$, then
$$
(\mathsf{T-\lambda I})(w)=\mathsf{TU}(v)-\lambda w=\mathsf{U}\mathsf{T}(v)-\lambda\mathsf{U}(v)=\mathsf{U}(\mathsf{T}(v)-\lambda v)=0
$$
thus $w\in\mathsf{E}_{\lambda}$ and so $\mathsf{E}_{\lambda}$ is $\mathsf{U}$-invariant, since $\mathsf{U}$ is diagonalizable, by Exercise 24, $\mathsf{U}_{\mathsf{E}_{\lambda}}$ is also diagonalizable, thus there exists a basis for $\mathsf{E}_{\lambda}$ consisting of eigenvectors of $\mathsf{U}$.

Suppose $\mathsf{T}$ has distinct eigenvalues $\lambda_1,\dots,\lambda_k$, for each $\lambda_i$ choose a basis for $\mathsf{E}_{\lambda_i}$ consisting of eigenvectors of $\mathsf{U}$ as above, put the bases together to get a basis $\beta$ for $\mathsf{V}$, then it is easy to see $[\mathsf{T}]_{\beta}$ and $[\mathsf{U}]_{\beta}$ are diagonal.

#### 26

Let $\lambda_1,\dots,\lambda_n$ be $n$ distinct eigenvalues of $\mathsf{T}$, and let $v_i$ be an eigenvector of $\mathsf{T}$ corresponding to $\lambda_i$ for $i=1,\dots,n$. Let $v=v_1+\cdots+v_n$, suppose there are scalars $c_1,\dots,c_n$ such that
$$
c_1v+c_2\mathsf{T}(v)+\cdots+c_n\mathsf{T}^{n-1}(v)=0
$$
then we have
$$
\begin{aligned}
&c_1(v_1+\cdots+v_n)
\\+&c_2(\lambda_1v_1+\cdots+\lambda_nv_n)
\\&\qquad{\vdots}
\\+&c_n(\lambda_1^{n-1}v_1+\cdots+\lambda_n^{n-1}v_n)=0
\end{aligned}\implies\begin{cases}
c_1+c_2\lambda_1+\cdots+c_n\lambda_1^{n-1}=0
\\
c_1+c_2\lambda_2+\cdots+c_n\lambda_2^{n-1}=0
\\\qquad{\vdots}\\
c_1+c_2\lambda_n+\cdots+c_n\lambda_n^{n-1}=0
\end{cases}
$$
the system of equations is a system of $n$ equations with $n$ unknowns, the coefficient matrix is a Vandermonde matrix, thus $c_1=\cdots=c_n=0$, which means $\{v,\mathsf{T}(v),\dots,\mathsf{T}^{n-1}(v)\}$ is linearly independent.

#### 27

(a) If $v+\mathsf{W}=v'+\mathsf{W}$, then $v-v'\in\mathsf{W}$, this means $\mathsf{T}(v-v')=\mathsf{T}(v)-\mathsf{T}(v')\in\mathsf{W}$, thus
$$
\overline{\mathsf{T}}(v+\mathsf{W})=\mathsf{T}(v)+\mathsf{W}=\mathsf{T}(v')+\mathsf{W}=\overline{\mathsf{T}}(v'+\mathsf{W})
$$
(b) We have
$$
\begin{aligned}
\overline{\mathsf{T}}(a(v+\mathsf{W})+u+\mathsf{W})&=\overline{\mathsf{T}}(av+u+\mathsf{W})
\\&=\mathsf{T}(av+u)+\mathsf{W}
\\&=a\mathsf{T}(v)+\mathsf{T}(u)+\mathsf{W}
\\&=a(\mathsf{T}(v)+\mathsf{W})+(\mathsf{T}(u)+\mathsf{W})
\\&=a\overline{\mathsf{T}}(v+\mathsf{W})+\overline{\mathsf{T}}(u+\mathsf{W})
\end{aligned}
$$
(c) Let any $v\in\mathsf{V}$​, we have
$$
(\eta\mathsf{T})(v)=\eta(\mathsf{T}(v))=\mathsf{T}(v)+\mathsf{W}=\overline{\mathsf{T}}(v+\mathsf{W})=\overline{\mathsf{T}}(\eta(v))=(\overline{\mathsf{T}}\eta)(v)
$$

#### 28

Extend an ordered basis $\gamma=\{v_1,v_2,\dots,v_k\}$ for $\mathsf{W}$ to an ordered basis $\beta=\{v_1,v_2,\dots,v_k,v_{k+1},\dots,v_n\}$ for $\mathsf{V}$, consider the collection $\alpha=\{v_{k+1}+\mathsf{W},\dots,v_n+\mathsf{W}\}$, use Exercise 35 of Section 1.6, $\alpha$ is a basis for $\mathsf{V}/\mathsf{W}$. And we can easily see from $\mathsf{W}$ being a $\mathsf{T}$-invariant space that
$$
[\mathsf{T}]_{\beta}=\begin{pmatrix}B_1&B_2\\O&B_3\end{pmatrix},\quad B_1=[\mathsf{T_W}]_{\gamma}
$$
Notice that for $j=k+1,\dots,n$, if we let
$$
\mathsf{T}(v_j)=c_{1j}v_1+\cdots+c_{nj}v_n
$$
then as $v_1,\dots,v_k\in\mathsf{W}$, we have
$$
\overline{\mathsf{T}}(v_j+\mathsf{W})=\mathsf{T}(v_j)+\mathsf{W}=c_{k+1,j}v_{k+1}+\cdots+c_{nj}v_n+\mathsf{W}=\sum_{i=k+1}^nc_{ij}(v_i+\mathsf{W})
$$
this gives $B_3=[\overline{\mathsf{T}}]_{\alpha}$. Now
$$
f(t)=\det([\mathsf{T}]_{\beta}-tI_n)=\det(B_1-tI_k)\det(B_3-tI_{n-k})=g(t)h(t)
$$

#### 29

That $\mathsf{T}$ is diagonalizable means for distinct eigenvalues $\lambda_1,\dots,\lambda_m$ of $\mathsf{T}$, we have $\mathsf{V}=\mathsf{E}_{\lambda_1}\oplus\cdots\oplus\mathsf{E}_{\lambda_m}$. Exercise 24 says that $\mathsf{T_W}$ is also diagonalizable, choose a basis $\gamma=\{v_1,v_2,\dots,v_k\}$ for $\mathsf{W}$ consisting of eigenvectors of $\mathsf{T_W}$, then each $v_j$ is also an eigenvector of $\mathsf{T}$ and thus belong to some $\mathsf{E}_{\lambda_i}$. we extend $\gamma$ to a basis for $\mathsf{V}$ by extending the vectors $\gamma\cap\mathsf{E}_{\lambda_i}$ to a basis for $\mathsf{E}_{\lambda_i}$, with the notation in the hints in Exercise 28, the added vectors are $\{v_{k+1},\dots,v_n\}$, since each $v_j$ belongs to some $\mathsf{E}_{\lambda}$ for $j=k+1,\dots,n$, we have
$$
\overline{\mathsf{T}}(v_j+\mathsf{W})=\mathsf{T}(v_j)+\mathsf{W}=\lambda v_j+\mathsf{W}=\lambda(v_j+\mathsf{W})
$$
thus the collection $\alpha=\{v_{k+1}+\mathsf{W},\dots,v_n+\mathsf{W}\}$ is a basis for $\mathsf{V}/\mathsf{W}$ consisting of eigenvectors for $\overline{\mathsf{T}}$, so $\overline{\mathsf{T}}$ is diagonalizable.

#### 30

Let $f(t),g(t),h(t)$ be the characteristic polynomials of $\mathsf{T},\mathsf{T_W}$ and $\overline{\mathsf{T}}$ respectively, then $g(t)$ and $h(t)$ both splits, which means $f(t)$ splits. Let $\lambda$ be an eigenvalue of $\mathsf{T}$, and the multiplicity is $m$, then $\lambda$ is either an eigenvalue of $\mathsf{T_W}$ or an eigenvalue of $\overline{\mathsf{T}}$. Then it is easy to deduce that the dimension of $\mathsf{E}_{\lambda}$ is equal to $m$, since both $\mathsf{T_W}$ and $\overline{\mathsf{T}}$ are diagonalizable.

#### 31

(a) We have
$$
\mathsf{T}(e_1)=Ae_1=\begin{pmatrix}1\\2\\1\end{pmatrix},\quad\mathsf{T}^2(e_1)=A\begin{pmatrix}1\\2\\1\end{pmatrix}=\begin{pmatrix}0\\12\\6\end{pmatrix}
$$
Since $6\mathsf{T}(e_1)-6e_1=\mathsf{T}^2(e_1)$, we have $\mathsf{T}^2(e_1)-6\mathsf{T}(e_1)+6e_1=0$, by Theorem 5.22, the characteristic polynomial of $\mathsf{T_W}$ is $f(t)=6-6t+t^2$.

(b) We have $\{e_1,\mathsf{T}(e_1)\}$ a basis for $\mathsf{W}$, thus $\mathsf{R^3/W}$ has dimension 1, it is easy to verify that $e_2\notin\mathsf{W}$, thus $\{e_2+\mathsf{W}\}$ is a basis for $\mathsf{R^3/W}$​, then we have
$$
\overline{\mathsf{T}}(e_2+\mathsf{W})=Ae_2+\mathsf{W}=\begin{pmatrix}1\\3\\2\end{pmatrix}+\mathsf{W}=-\begin{pmatrix}0\\1\\0\end{pmatrix}-\begin{pmatrix}1\\0\\0\end{pmatrix}+2\begin{pmatrix}1\\2\\1\end{pmatrix}+\mathsf{W}
$$
or $\overline{\mathsf{T}}(e_2+\mathsf{W})=-e_2-e_1+2\mathsf{T}(e_1)+\mathsf{W}$, this equals to $\overline{\mathsf{T}}(e_2+\mathsf{W})=-(e_2+\mathsf{W})$, so the characteristic polynomial of $\overline{\mathsf{T}}$ is $g(t)=-1-t$.

(c) The characteristic polynomial of $A$ is $f(t)g(t)=-(t+1)(t^2-6t+6)$​.

#### 32

Use induction to $\dim(\mathsf{V})$, when $\dim(\mathsf{V})=1$, $\beta$ has only one vector, and $[\mathsf{T}]_{\beta}$ is a $1\times 1$ matrix, which is upper triangular. Suppose the conclusion is true when $\dim(\mathsf{V})=k$, consider the case when $\dim(\mathsf{V})=k+1$. Since the characteristic polynomial of $\mathsf{T}$ splits, $\mathsf{T}$ has at least one eigenvalue, let $v$ be an eigenvector of $\mathsf{T}$ corresponding to eigenvalue $\lambda$ and $\mathsf{W}=\text{span}(\{v\})$, consider the linear transformation $\overline{\mathsf{T}}$, use Exercise 35(b) of Section 1.6 we know that $\dim(\mathsf{V/W})=k$, and use Exercise 28 we see the characteristic polynomial of $\overline{\mathsf{T}}$ splits, by induction hypothesis, there is an ordered basis $\beta'=\{v_1+\mathsf{W},\dots,v_k+\mathsf{W}\}$ for $\mathsf{V/W}$ and $[\overline{\mathsf{T}}]_{\beta'}$ is an upper triangular matrix. 

Let $a_0,a_1,\dots,a_k$ be scalars such that $a_0v+\sum_{i=1}^ka_iv_i=0$, thus $\sum_{i=1}^ka_iv_i=-a_0v\in\mathsf{W}$, which means
$$
\sum_{i=1}^ka_i(v_i+\mathsf{W})=\sum_{i=1}^ka_iv_i+\mathsf{W}=-a_0v+\mathsf{W}=0+\mathsf{W}\implies a_1=\cdots=a_k=0
$$
and so $a_0v=-\sum_{i=1}^ka_iv_i=0$, which means $a_0=0$ as $v$ is an eigenvector of $\mathsf{T}$. We proved the set of vectors $\beta=\{v,v_1,\dots,v_k\}$ is linearly independent in $\mathsf{V}$, thus a basis in $\mathsf{V}$​. Use the hint of Exercise 28, we see that
$$
[\mathsf{T}]_{\beta}=\begin{pmatrix}\lambda&B_2\\O&[\overline{\mathsf{T}}]_{\beta'}\end{pmatrix}
$$
this is an upper triangular matrix.

### Summary

本节讨论不变子空间，不变子空间的概念在前序章节的习题中已经介绍过，cyclic subspace是一种特殊的不变子空间。定理5.21说明，不变子空间包含了T的一些局部性质（例如特征多项式的因式），因此利用不变子空间可以收集T的一些信息，而T限制在一个cyclic subspace上的特征多项式容易计算。定理5.22说明了如何寻找cyclic subspace的一组基，以及如何快速的计算cyclic subspace的特征多项式。定理5.23（Cayley-Hamilton）进一步说明定理5.22的重要性，即T是“满足”自身特征多项式的