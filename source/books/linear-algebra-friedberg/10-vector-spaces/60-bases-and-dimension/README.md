# Bases and Dimension

### Content Notes

#### Definition (basis)

A basis $\beta$ for a vector space $\mathsf{V}$ is a linearly independent subset of $\mathsf{V}$ that generates $\mathsf{V}$. If $\beta$ is a basis for $\mathsf{V}$, we also say that the vectors of $\beta$ form a basis for $\mathsf{V}$.

#### Theorem 1.8

Let $\mathsf{V}$ be a vector space and $\beta=\{u_1,u_2,\dots,u_n\}$ be a subset of $\mathsf{V}$. Then $\beta$ is a basis for $\mathsf{V}$ if and only if each $v\in\mathsf{V}$ can be uniquely expressed as a linear combination of vectors of $\beta$, that is, can be expressed in the form
$$
v=a_1u_1+\cdots+a_nu_n
$$
for unique scalars $a_1,\dots,a_n$.

#### Theorem 1.9

If a vector space $\mathsf{V}$ is generated by a finite set $S$, then some subset of $S$ is a basis for $\mathsf{V}$. Hence $\mathsf{V}$ has a finite basis.

#### Theorem 1.10 (Replacement Theorem)

Let $\mathsf{V}$ be a vector space that is generated by a set $G$ containing exactly $n$ vectors, and let $L$ be a linearly independent subset of $\mathsf{V}$ containing exactly $m$ vectors. Then $m\le n$ and there exists a subset $H$ of $G$ containing exactly $n-m$ vectors such that $L\cup H$ generates $\mathsf{V}$.

##### Corollary 1

Let $\mathsf{V}$ be a vector space having a finite basis. Then every basis for $\mathsf{V}$ contains the same number of vectors.

#### Definitions (finite-dimensional, dimension, infinite-dimensional)

A vector space is called **finite-dimensional** if it has a basis consisting of a finite number of vectors. The unique number of vectors in each basis for $\mathsf{V}$ is called the **dimension** of $\mathsf{V}$ and is denoted by $\dim(\mathsf{V})$. A vector space that is not finite-dimensional is called **infinite-dimensional**.  

##### Corollary 2

Let $\mathsf{V}$ be a vector space with dimension $n$.

- (a)  Any finite generating set for $\mathsf{V}$ contains at least $n$ vectors, and a generating set for $\mathsf{V}$ that contains exactly $n$ vectors is a basis for $\mathsf{V}$.
- (b) Any linearly independent subset of $\mathsf{V}$ that contains exactly $n$ vectors is a basis for $\mathsf{V}$.
- (c) Every linearly independent subset of $\mathsf{V}$ can be extended to a basis for $\mathsf{V}$.

#### Theorem 1.11

Let $\mathsf{W}$ be a subspace of a finite-dimensional vector space $\mathsf{V}$. Then $\mathsf{W}$ is finite-dimensional and $\dim(\mathsf{W})\le\dim(\mathsf{V})$. Moreover, if $\dim(\mathsf{W})=\dim(\mathsf{V})$, then $\mathsf{V}=\mathsf{W}$.

##### Corollary

If $\mathsf{W}$ is a subspace of a finite-dimensional vector space $\mathsf{V}$, then any basis for $\mathsf{W}$ can be extended to a basis for $\mathsf{V}$.

### Selected Exercises

#### 15

$\dim(\mathsf{W})=n^2-1$ and a basis of $\mathsf{W}$ can be $E_{ij}$ with $i\ne j$, together with the matrix
$$
\begin{pmatrix}1&&\\&{\ddots}&\\&&-1\end{pmatrix},\cdots\begin{pmatrix}0&&&\\&{\ddots}&&\\&&1&\\&&&-1\end{pmatrix}
$$

#### 16

$\dim(\mathsf{W})=1+2+\cdots+n=\dfrac{n(n+1)}{2}$, a basis of $\mathsf{W}$ can be $E_{ij}$ with $i\le j$.

#### 17

$\dim(\mathsf{W})=\dfrac{n(n-1)}{2}$, a basis of $\mathsf{W}$ can be $\delta_{ij}$ with $i<j$, each $\delta_{ij}$ has $1$ in the $i$th row and the $j$th column, $-1$ in the $j$th row and the $i$th column, and $0$ in other positions.

#### 19

Since each $v\in\mathsf{V}$ can be expressed as a linear combination of vectors in $\beta$, we see $\beta$ generates $\mathsf{V}$. To see that $\beta$ is linearly independent, assume not, then one of the vectors $u_i$ can be generated by others, that is
$$
u_i=c_1u_1+\cdots+c_{i-1}u_{i-1}+c_{i+1}u_{i+1}+\cdots+c_nu_n
$$
this means
$$
c_1u_1+\cdots+c_{i-1}u_{i-1}+(-1)u_i+c_{i+1}u_{i+1}+\cdots+c_nu_n=0
$$
since we also have
$$
0u_1+\cdots+0u_{i-1}+0u_i+0u_{i+1}+\cdots+0u_n=0
$$
this contradicts the unique representation of $0\in\mathsf{V}$ as linear combinations of vectors in $\beta$, this means $\beta$ is linearly independent.

#### 20

(a) $\mathsf{V}$ has dimension $n$ means there is a basis $v_1,\dots,v_n$ of $\mathsf{V}$. Since $S$ generates $\mathsf{V}$, for each $v_i$ we can find some finite number of vectors in $S$ that $v_i$ is a linear combination of these vectors, combine all these vectors from $v_1$ to $v_n$, all are vectors in $S$, to form a subset of $S$, this finite subset of $S$ generates $\mathsf{V}$, and thus can be reduced to a basis of $\mathsf{V}$.

(b) Use corollary 2 of Theorem 1.10.

#### 21

If it contains an infinite linearly independent subset, then it can be extended to a basis, thus the space cannot be finite-dimensional.

If it is infinite-dimensional, and assume all linearly independent subsets are finite, then it means the space is finite dimensional, a contradiction.

#### 22

$\mathsf{W_1}\subseteq\mathsf{W_2}$.

#### 23

(a) $v\in\mathsf{W_1}$.

(b) $\dim(\mathsf{W_2})=\dim(\mathsf{W_1})+1$.

#### 24

That $f^{(k)}(x)$ has degree $n-k$, so $\{f(x),f'(x),\dots,f^{(n)}(x)\}$ is a basis for $\mathsf{P}_n(R)$.

#### 25

$\dim(Z)=mn$. (Cartesian products)

#### 26

All polynomials in this subspace shall have the factor $x-a$ in it, thus it is equivalent to the space $\mathsf{P}_{n-1}(R)$ and has dimension $n$.

#### 29

(a) Just follow the hint.

(b) $\mathsf{V}$ is the direct sum of $\mathsf{W_1}$ and $\mathsf{W_2}$ $\Leftrightarrow$ $\mathsf{W_1}\cap\mathsf{W_2}=\{0\}\Leftrightarrow\dim(\mathsf{W_1}\cap\mathsf{W_2})=0$, then use the equation in (a), which is
$$
\dim(\mathsf{W_1}+\mathsf{W_2})=\dim(\mathsf{W_1})+\dim(\mathsf{W_2})-\dim(\mathsf{W_1}\cap\mathsf{W_2})
$$

#### 31

(a) $\mathsf{W_1}\cap\mathsf{W_2}\subseteq\mathsf{W_2}$.

(b) $\dim(\mathsf{W_1}+\mathsf{W_2})=m+n-\dim(\mathsf{W_1}\cap\mathsf{W_2})\le m+n$.

#### 32

(a) $\mathsf{W_1}=R^3$ and $\mathsf{W_2}=\{(a,0,0),a\in R\}$.

(b) $\mathsf{W_1}=\{(0,a,b),a,b\in R\}$ and $\mathsf{W_2}=\{(a,0,0),a\in R\}$.

(c) $\mathsf{W_1}=\{(0,a,b),a,b\in R\}$ and $\mathsf{W_2}=\{(a,b,0),a,b\in R\}$.

#### 33

(a) If $\beta_1\cap \beta_2\ne\empty$, then $\dim(\mathsf{W_1}\cap\mathsf{W_2})\ge 1$ and contradicts $\mathsf{V}=\mathsf{W_1}\oplus\mathsf{W_2}$.

To prove $\beta_1\cup\beta_2$ is a basis for $\mathsf{V}$, first prove it spans $\mathsf{V}$, for any $v\in\mathsf{V}$, we can write $v=a+b$ where $a\in\mathsf{W_1},b\in\mathsf{W_2}$, then $a$ is a linear combination of some vectors in $\beta_1$ and $b$ is a linear combination of some vectors in $\beta_2$, which shows $v$ is a linear combination of some vectors in $\beta_1\cup\beta_2$. Next prove it is linearly independent. Assume not, then we can find distinct vectors $u_1,\dots,u_n\in\beta_1\cup\beta_2$ such that
$$
a_1u_1+\cdots+a_nu_n=0
$$
for $a_1,\dots,a_n$ not all zero. Since $\beta_1\cap\beta_2=\empty$, we may rearrange the index of the $u_i$s such that $u_1\dots,u_k\in\beta_1$ and $u_{k+1},\dots,u_n\in\beta_2$, let
$$
w_1=a_1u_1+\cdots+a_ku_k\in\mathsf{W_1},\quad w_2=a_{k+1}u_{k+1}+\cdots+a_nu_n\in\mathsf{W_2}
$$
we get $w_1+w_2=0$, and $\mathsf{V}=\mathsf{W_1}\oplus\mathsf{W_2}$ gives $w_1=w_2=0$. Then use $\beta_1$ and $\beta_2$ are bases, thus linearly independent, we have all $a_i$s be zero, a contradiction.

(b) $\beta_1\cup\beta_2$ is a basis for $\mathsf{V}$ means $\mathsf{V}=\mathsf{W_1}+\mathsf{W_2}$. Suppose $v\in\mathsf{W_1}\cap\mathsf{W_2}$, then $v$ can be expressed as linear combination of vectors in $\beta_1$ and linear combination of vectors in $\beta_2$. Write
$$
v=a_1u_1+\cdots+a_nu_n=b_1v_1+\cdots+b_mv_m
$$
in which $u_1,\dots,u_n\in \beta_1$ and $v_1,\dots,v_m\in\beta_2$, this means
$$
a_1u_1+\cdots+a_nu_n-b_1v_1-\cdots-b_mv_m=0
$$
thus $a_1=\cdots=a_n=b_1=\cdots=b_m=0$. So $v=0$.

#### 34

(a) Choose a basis for $\mathsf{W_1}$, extend it to a basis of $\mathsf{V}$, the subspace spanned by the added vectors can be $\mathsf{W_2}$.

(b) $\mathsf{W_2}=\{(0,b):b\in R\}$, and $\mathsf{W_2'}=\{(b,b):b\in R\}$.

#### 35

(a) For any $v+\mathsf{W}\in\mathsf{V}/\mathsf{W}$, write $v=a_1u_1+\cdots+a_nu_n$, then we claim that
$$
a_{k+1}(u_{k+1}+\mathsf{W})+\cdots+a_n(u_n+\mathsf{W})=\left(\sum_{j=k+1}^na_ju_j+\mathsf{W}\right)=v+\mathsf{W}
$$
The last equality comes from the fact that $v-\sum_{j=k+1}^na_ju_j=\sum_{j=1}^ka_ju_j\in\mathsf{W}$. This shows that $\{u_{k+1}+\mathsf{W},\dots,u_n+\mathsf{W}\}$ spans $\mathsf{V}/\mathsf{W}$.

To prove linearly independence of this set, let
$$
a_{k+1}(u_{k+1}+\mathsf{W})+\cdots+a_n(u_n+\mathsf{W})=\sum_{j=k+1}^na_ju_j+\mathsf{W}=0+\mathsf{W}
$$
which means $\sum_{j=k+1}^na_ju_j-0\in\mathsf{W}$, so we can find $a_1,\dots,a_k$​ such that
$$
\sum_{j=k+1}^na_ju_j=\sum_{j=1}^ka_ju_j\implies\sum_{j=1}^ka_ju_j-\sum_{j=k+1}^na_ju_j=0
$$
this means $a_{k+1}=\cdots=a_n=0$, thus $\{u_{k+1}+\mathsf{W},\dots,u_n+\mathsf{W}\}$ is linearly independent.

(b) $\dim(\mathsf{W})+\dim(\mathsf{V}/\mathsf{W})=\dim(\mathsf{V})$.

### Summary

本节介绍基和维数，是线性代数中相对核心的概念。基（basis）的概念建立是基于张成空间和线性无关，实质是选择“最小的”能代表空间中所有向量的向量组，这个向量组不唯一但数量是恒定的。这个恒定的数量只和空间有关，即该空间的维数。